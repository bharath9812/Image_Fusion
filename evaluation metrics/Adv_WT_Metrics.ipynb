{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b12f87c-9817-4c27-b891-fab29013bb35",
   "metadata": {},
   "source": [
    "## Reference Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eff80987-3d87-4a8e-8ee3-e9cf6567a775",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your images\n",
    "img1 = cv2.imread('D:/SDP/Imfusion-main/screenshots/medical1.png', cv2.IMREAD_GRAYSCALE)\n",
    "img2 = cv2.imread('D:/SDP/Imfusion-main/screenshots/medical2.png', cv2.IMREAD_GRAYSCALE)\n",
    "fused_img = cv2.imread('D:/SDP/Imfusion-main/Fused_Images/fused.png', cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "def resize_images(*images):\n",
    "    # Find the smallest dimensions in the set\n",
    "    min_height = min(image.shape[0] for image in images)\n",
    "    min_width = min(image.shape[1] for image in images)\n",
    "    resized_images = [cv2.resize(img, (min_width, min_height), interpolation=cv2.INTER_AREA) for img in images]\n",
    "    return resized_images\n",
    "\n",
    "\n",
    "img1, img2, fused_img = resize_images(img1, img2, fused_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "acbfe3c7-69d1-4e21-ba4a-bc8f4990ed65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QNCIE value: 0.922940599642658\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_ncc(image1, image2):\n",
    "    \"\"\"\n",
    "    Compute the Nonlinear Correlation Coefficient (NCC) between two images.\n",
    "    \n",
    "    Parameters:\n",
    "        image1: numpy.ndarray, source image 1\n",
    "        image2: numpy.ndarray, source image 2\n",
    "    \n",
    "    Returns:\n",
    "        float, NCC value\n",
    "    \"\"\"\n",
    "    mean1 = np.mean(image1)\n",
    "    mean2 = np.mean(image2)\n",
    "    std1 = np.std(image1)\n",
    "    std2 = np.std(image2)\n",
    "    \n",
    "    NCC = np.mean((image1 - mean1) * (image2 - mean2)) / (std1 * std2)\n",
    "    \n",
    "    return NCC\n",
    "\n",
    "def compute_ncc_matrix(img1, img2, fused_img):\n",
    "    \"\"\"\n",
    "    Compute the nonlinear correlation matrix R using NCC values.\n",
    "    \n",
    "    Parameters:\n",
    "        img1: numpy.ndarray, source image 1\n",
    "        img2: numpy.ndarray, source image 2\n",
    "        fused_img: numpy.ndarray, fused image\n",
    "    \n",
    "    Returns:\n",
    "        numpy.ndarray, nonlinear correlation matrix R\n",
    "    \"\"\"\n",
    "    NCCA_B = compute_ncc(img1, img2)\n",
    "    NCCA_F = compute_ncc(img1, fused_img)\n",
    "    NCCB_A = compute_ncc(img2, img1)\n",
    "    NCCB_F = compute_ncc(img2, fused_img)\n",
    "    NCCF_A = compute_ncc(fused_img, img1)\n",
    "    NCCF_B = compute_ncc(fused_img, img2)\n",
    "    \n",
    "    R = np.array([[1, NCCA_B, NCCA_F],\n",
    "                  [NCCB_A, 1, NCCB_F],\n",
    "                  [NCCF_A, NCCF_B, 1]])\n",
    "    \n",
    "    return R\n",
    "\n",
    "def compute_qncie(img1, img2, fused_img):\n",
    "    \"\"\"\n",
    "    Compute the Nonlinear Correlation Information Entropy (QNCIE) between source images and fused image.\n",
    "    \n",
    "    Parameters:\n",
    "        img1: numpy.ndarray, source image 1\n",
    "        img2: numpy.ndarray, source image 2\n",
    "        fused_img: numpy.ndarray, fused image\n",
    "    \n",
    "    Returns:\n",
    "        float, QNCIE value\n",
    "    \"\"\"\n",
    "    R = compute_ncc_matrix(img1, img2, fused_img)\n",
    "    eigenvalues = np.linalg.eigvals(R)\n",
    "    qncie = 1 + np.sum((eigenvalues/3) * np.log2(eigenvalues/3)) / np.log2(256)\n",
    "    \n",
    "    return qncie\n",
    "\n",
    "# Example usage:\n",
    "qncie_value = compute_qncie(img1, img2, fused_img)\n",
    "print(\"QNCIE value:\", qncie_value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30dc6be5-c6b6-492c-9ba0-34204daef38c",
   "metadata": {},
   "source": [
    "The QNCIE value of 0.9229 suggests a high degree of nonlinear correlation between the fused image and the source images. In other words, the fusion process has effectively preserved the nonlinear relationships present in the original images.\n",
    "\n",
    "In terms of image fusion quality, this high QNCIE value indicates that the fused image captures the essential features and characteristics of both source images while maintaining strong correlations with them. Therefore, the overall impression is that the fusion image quality is excellent, as it effectively combines information from the source images while preserving their nonlinear relationships.\n",
    "\n",
    "    Low QNCIE (0 to 0.3):\n",
    "        Indicates poor fusion quality.\n",
    "        The fused image may not effectively capture the essential features of the source images.\n",
    "        There is minimal correlation between the fused image and the source images, suggesting a lack of information preservation.\n",
    "\n",
    "    Moderate QNCIE (0.3 to 0.7):\n",
    "        Suggests moderate fusion quality.\n",
    "        The fused image retains some features from the source images, but there may be some loss of detail or fidelity.\n",
    "        Correlation between the fused image and the source images is moderate, indicating partial preservation of information.\n",
    "\n",
    "    High QNCIE (0.7 to 1):\n",
    "        Indicates excellent fusion quality.\n",
    "        The fused image effectively combines information from the source images while preserving their essential features and characteristics.\n",
    "        There is strong correlation between the fused image and the source images, suggesting a high degree of information preservation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9b913790-9d01-4663-a37c-b39de06e05a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FMI value: 3.0720635497365\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import mutual_info_regression\n",
    "\n",
    "def compute_mi(image1, image2):\n",
    "    \"\"\"\n",
    "    Compute Mutual Information (MI) between two images.\n",
    "    \n",
    "    Parameters:\n",
    "        image1: numpy.ndarray, source image 1\n",
    "        image2: numpy.ndarray, source image 2\n",
    "    \n",
    "    Returns:\n",
    "        float, MI value\n",
    "    \"\"\"\n",
    "    # Flatten images to 1D arrays\n",
    "    flat_image1 = image1.flatten()\n",
    "    flat_image2 = image2.flatten()\n",
    "    \n",
    "    # Compute mutual information\n",
    "    mi = mutual_info_regression(flat_image1.reshape(-1, 1), flat_image2)[0]\n",
    "    \n",
    "    return mi\n",
    "\n",
    "def compute_fmi(img1, img2, fused_img):\n",
    "    \"\"\"\n",
    "    Compute the Feature Mutual Information (FMI) between source images and fused image.\n",
    "    \n",
    "    Parameters:\n",
    "        img1: numpy.ndarray, source image 1\n",
    "        img2: numpy.ndarray, source image 2\n",
    "        fused_img: numpy.ndarray, fused image\n",
    "    \n",
    "    Returns:\n",
    "        float, FMI value\n",
    "    \"\"\"\n",
    "    # Compute Mutual Information between each source image and fused image\n",
    "    mi_af = compute_mi(img1, fused_img)\n",
    "    mi_bf = compute_mi(img2, fused_img)\n",
    "    \n",
    "    # Sum up the Mutual Information values\n",
    "    fmi = mi_af + mi_bf\n",
    "    \n",
    "    return fmi\n",
    "\n",
    "# Example usage:\n",
    "fmi_value = compute_fmi(img1, img2, fused_img)\n",
    "print(\"FMI value:\", fmi_value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a938243-be22-47fb-8073-aa58dad8211e",
   "metadata": {},
   "source": [
    "\n",
    "    Low FMI (0 to 1):\n",
    "        Indicates poor fusion quality.\n",
    "        The fused image may lack important characteristic information from the source images.\n",
    "        There is minimal transmission of features to the fused image, resulting in a loss of distinctive characteristics.\n",
    "\n",
    "    Moderate FMI (1 to 2):\n",
    "        Suggests moderate fusion quality.\n",
    "        The fused image retains some characteristic information from the source images, but there may be some loss of detail or clarity.\n",
    "        Features from the source images are partially transmitted to the fused image, resulting in a moderate fusion effect.\n",
    "\n",
    "    High FMI (2 and above):\n",
    "        Indicates excellent fusion quality.\n",
    "        The fused image effectively combines characteristic information from the source images.\n",
    "        There is a significant transmission of features to the fused image, resulting in a high-quality fusion effect.\n",
    "\n",
    "In summary, a higher FMI value indicates better fusion quality, as it signifies a greater transmission of characteristic information from the source images to the fused image. In this case, the FMI value of 3.072 suggests a high-quality fusion with a significant transmission of features, resulting in an effective combination of information from the source images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fa438704-352e-486e-9f8c-19a1a635160f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NMI value: 3.0368034191887237\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_entropy(image):\n",
    "    \"\"\"\n",
    "    Compute the entropy of an image.\n",
    "    \n",
    "    Parameters:\n",
    "        image: numpy.ndarray, input image\n",
    "    \n",
    "    Returns:\n",
    "        float, entropy value\n",
    "    \"\"\"\n",
    "    # Flatten image to 1D array\n",
    "    flat_image = image.flatten()\n",
    "    \n",
    "    # Compute histogram\n",
    "    hist, _ = np.histogram(flat_image, bins=256, density=True)\n",
    "    \n",
    "    # Remove zeros to avoid log(0) issues\n",
    "    hist = hist[hist != 0]\n",
    "    \n",
    "    # Compute entropy\n",
    "    entropy = -np.sum(hist * np.log2(hist))\n",
    "    \n",
    "    return entropy\n",
    "\n",
    "def compute_mi(image1, image2):\n",
    "    \"\"\"\n",
    "    Compute Mutual Information (MI) between two images.\n",
    "    \n",
    "    Parameters:\n",
    "        image1: numpy.ndarray, source image 1\n",
    "        image2: numpy.ndarray, source image 2\n",
    "    \n",
    "    Returns:\n",
    "        float, MI value\n",
    "    \"\"\"\n",
    "    # Flatten images to 1D arrays\n",
    "    flat_image1 = image1.flatten()\n",
    "    flat_image2 = image2.flatten()\n",
    "    \n",
    "    # Compute joint histogram\n",
    "    joint_hist, _, _ = np.histogram2d(flat_image1, flat_image2, bins=256, density=True)\n",
    "    \n",
    "    # Remove zeros to avoid log(0) issues\n",
    "    joint_hist = joint_hist[joint_hist != 0]\n",
    "    \n",
    "    # Compute mutual information\n",
    "    mi = -np.sum(joint_hist * np.log2(joint_hist))\n",
    "    \n",
    "    return mi\n",
    "\n",
    "def compute_nmi(img1, img2, fused_img):\n",
    "    \"\"\"\n",
    "    Compute the Normalized Mutual Information (NMI) between source images and fused image.\n",
    "    \n",
    "    Parameters:\n",
    "        img1: numpy.ndarray, source image 1\n",
    "        img2: numpy.ndarray, source image 2\n",
    "        fused_img: numpy.ndarray, fused image\n",
    "    \n",
    "    Returns:\n",
    "        float, NMI value\n",
    "    \"\"\"\n",
    "    # Compute entropy of each image\n",
    "    entropy_a = compute_entropy(img1)\n",
    "    entropy_b = compute_entropy(img2)\n",
    "    entropy_f = compute_entropy(fused_img)\n",
    "    \n",
    "    # Compute mutual information between each source image and fused image\n",
    "    mi_af = compute_mi(img1, fused_img)\n",
    "    mi_bf = compute_mi(img2, fused_img)\n",
    "    \n",
    "    # Compute NMI\n",
    "    nmi = (2 * (mi_af / (entropy_a + entropy_f))) + (2 * (mi_bf / (entropy_b + entropy_f)))\n",
    "    \n",
    "    return nmi\n",
    "\n",
    "# Example usage:\n",
    "nmi_value = compute_nmi(img1, img2, fused_img)\n",
    "print(\"NMI value:\", nmi_value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2239d68-4be5-41ea-9976-bd77181a6ed4",
   "metadata": {},
   "source": [
    "The Normalized Mutual Information (NMI) value ranges and their corresponding interpretations are as follows:\n",
    "\n",
    "    Low NMI (0 to 1):\n",
    "        Indicates poor fusion quality.\n",
    "        The fused image may not effectively capture the essential features or characteristics of the source images.\n",
    "        There is minimal mutual information shared between the source images and the fused image.\n",
    "\n",
    "    Moderate NMI (1 to 2):\n",
    "        Suggests moderate fusion quality.\n",
    "        The fused image retains some mutual information from the source images, but there may be some loss of detail or fidelity.\n",
    "        Mutual information shared between the source images and the fused image is moderate, indicating partial preservation of information.\n",
    "\n",
    "    High NMI (2 and above):\n",
    "        Indicates excellent fusion quality.\n",
    "        The fused image effectively combines mutual information from the source images.\n",
    "        There is a significant amount of mutual information shared between the source images and the fused image, resulting in a high-quality fusion effect.\n",
    "\n",
    "In this case, the NMI value of 3.0368 suggests a high-quality fusion with a significant amount of mutual information shared between the source images and the fused image. Therefore, the overall impression is that the fusion quality is excellent, as the fused image effectively combines information from the source images while preserving their essential characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e29abe04-655f-434c-b0b2-6a1d36f7471f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'SSIM_CT': 0.3814543565102585, 'SSIM_MRI': 0.6364497066122207, 'MSE_CT': 1177.9404761904761, 'MSE_MRI': 1214.2030677655678, 'MI_CT': 1.4272309792791746, 'MI_MRI': 2.460728723690644}\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from skimage.metrics import mean_squared_error\n",
    "\n",
    "def mutual_information(image1, image2):\n",
    "    hist_2d, _, _ = np.histogram2d(image1.ravel(), image2.ravel(), bins=256)\n",
    "    pxy = hist_2d / float(np.sum(hist_2d))\n",
    "    px = np.sum(pxy, axis=1)\n",
    "    py = np.sum(pxy, axis=0)\n",
    "    px_py = px[:, None] * py[None, :]\n",
    "    nzs = pxy > 0\n",
    "    return np.sum(pxy[nzs] * np.log(pxy[nzs] / px_py[nzs]))\n",
    "\n",
    "def resize_images(*images):\n",
    "    # Find the smallest dimensions in the set\n",
    "    min_height = min(image.shape[0] for image in images)\n",
    "    min_width = min(image.shape[1] for image in images)\n",
    "    resized_images = [cv2.resize(img, (min_width, min_height), interpolation=cv2.INTER_AREA) for img in images]\n",
    "    return resized_images\n",
    "\n",
    "def calculate_metrics(img1, img2, fused_img):\n",
    "    metrics = {}\n",
    "    # Resize images to the smallest common dimensions\n",
    "    img1, img2, fused_img = resize_images(img1, img2, fused_img)\n",
    "    metrics['SSIM_CT'] = ssim(img1, fused_img, data_range=fused_img.max() - fused_img.min())\n",
    "    metrics['SSIM_MRI'] = ssim(img2, fused_img, data_range=fused_img.max() - fused_img.min())\n",
    "    metrics['MSE_CT'] = mean_squared_error(img1, fused_img)\n",
    "    metrics['MSE_MRI'] = mean_squared_error(img2, fused_img)\n",
    "    metrics['MI_CT'] = mutual_information(img1, fused_img)\n",
    "    metrics['MI_MRI'] = mutual_information(img2, fused_img)\n",
    "    # Add more metrics here if needed\n",
    "    return metrics\n",
    "\n",
    "# Load your images\n",
    "img1 = cv2.imread('D:/SDP/Imfusion-main/screenshots/medical1.png', cv2.IMREAD_GRAYSCALE)\n",
    "img2 = cv2.imread('D:/SDP/Imfusion-main/screenshots/medical2.png', cv2.IMREAD_GRAYSCALE)\n",
    "fused_img = cv2.imread('D:/SDP/Imfusion-main/Fused_Images/fused.png', cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "# Calculate the metrics\n",
    "results = calculate_metrics(img1, img2, fused_img)\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a8feb3-3e7e-49ff-93cf-61d35bbdbeda",
   "metadata": {},
   "source": [
    "1. Structural Similarity Index Measure (SSIM)\n",
    "SSIM for CT (0.381): This is relatively low, indicating that the structural similarity between the fused image and the original CT scan is not very high. This suggests that some structural details from the CT image are not well preserved in the fused image.\n",
    "SSIM for MRI (0.636): This is moderately high, showing that the fused image has a better preservation of structural details from the MRI scan compared to the CT scan. This suggests that the fusion process is more aligned with the characteristics of the MRI.\n",
    "\n",
    "\n",
    "3. Mean Square Error (MSE)\n",
    "MSE for CT (1177.94): A higher MSE value indicates a larger average squared difference between the pixel values of the fused image and the original CT image, which points to greater deviation and less accurate representation of the CT in the fusion.\n",
    "MSE for MRI (1214.20): Similarly high, indicating significant differences between the fused image and the MRI image. This suggests that while structural similarities might be somewhat retained (as seen in SSIM), the exact pixel values differ considerably.\n",
    "\n",
    "\n",
    "5. Mutual Information (MI)\n",
    "MI for CT (1.427): This value shows moderate mutual information between the fused image and the CT scan, indicating that some amount of shared information exists, but it's not particularly high.\n",
    "MI for MRI (2.461): This is higher compared to the CT, suggesting that the fused image shares more information with the MRI scan. This aligns with the SSIM results, further confirming that the MRI features are better represented in the fused image.\n",
    "\n",
    "\n",
    "Conclusion\n",
    "\n",
    "\n",
    "The metrics suggest that the fusion process tends to favor the MRI image over the CT scan in terms of preserving both structural and informational content. This could be due to inherent characteristics of the images, the specific settings of the fusion process, or the types of details (e.g., soft tissues in MRI vs. hard tissues in CT) that are better captured in MRI scans. The quality of the fusion, particularly with respect to the CT image, could be improved by adjusting the fusion algorithm to better balance the representation of both types of images.\n",
    "\n",
    "For better fusion quality:\n",
    "\n",
    "Consider using fusion techniques that can better handle the disparities in image characteristics between CT and MRI.\n",
    "Adjust parameters or explore different fusion rules that might enhance the preservation of CT image details.\n",
    "Experiment with preprocessing steps that might make the CT and MRI images more compatible for fusion, such as intensity or contrast adjustments.\n",
    "This analysis can guide further optimizations and adjustments to the image fusion process to achieve a more balanced and effective representation of both CT and MRI characteristics in the fused image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9800f8ca-d2b1-4c2e-8baa-340d19ef2a20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276bc78b-fed1-4ba4-8bc8-d1f9f081728e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a2fb18-0f51-4746-97d5-01ad6f2e294d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cf8597cd-f5af-436e-a897-4272e8ba02ba",
   "metadata": {},
   "source": [
    "## No-Reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1738c151-4b4b-46b9-afda-75b43531c82f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy: 5.897526264190674\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def calculate_entropy(image):\n",
    "    \"\"\"Calculate the entropy of an image.\"\"\"\n",
    "    # Convert image to grayscale if it's not already\n",
    "    if len(image.shape) == 3:\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Calculate histogram from 0 to 255\n",
    "    hist = cv2.calcHist([image], [0], None, [256], [0,256])\n",
    "    # Normalize the histogram\n",
    "    hist_norm = hist.ravel()/hist.sum()\n",
    "    # Calculate the entropy\n",
    "    logs = np.log2(hist_norm + 1e-10)  # Add a small value to avoid log(0)\n",
    "    entropy = -1 * (hist_norm * logs).sum()\n",
    "    \n",
    "    return entropy\n",
    "\n",
    "# Load your fused image\n",
    "fused_img = cv2.imread('D:/SDP/Imfusion-main/Fused_Images/fused.png', cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "# Calculate the entropy\n",
    "entropy_value = calculate_entropy(fused_img)\n",
    "print(f\"Entropy: {entropy_value}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab76a32-8e1b-46c5-a772-369f15da0e00",
   "metadata": {},
   "source": [
    "Summary of the Fused Image's Entropy:\n",
    "\n",
    "Value: An entropy of 5.90 indicates a moderate to high level of information content in the fused image. This suggests that the fusion process has managed to retain a good amount of information from the original CT and MRI images.\n",
    "\n",
    "Interpretation: Higher entropy values generally mean that the image contains a wide variety of pixel values, implying more texture or detail. This can be viewed as a positive aspect of the fusion process, indicating that it has not resulted in excessive information loss.\n",
    "\n",
    "Conclusion: Based on the entropy alone, the fusion effect can be considered effective to a certain degree. The image retains diverse information, which could be beneficial for further analysis or diagnostic purposes in medical imaging scenarios.\n",
    "\n",
    "This metric provides a useful standalone assessment of the fused image's quality in terms of information retention. However, for a comprehensive evaluation, it would be ideal to consider this metric in conjunction with other reference and no-reference metrics that assess different aspects of image quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da1355e1-31e3-4887-80a8-c7bd964cf48d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Gradient: 78.01037390623792\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def calculate_average_gradient(image):\n",
    "    \"\"\"Calculate the average gradient of an image.\"\"\"\n",
    "    # Ensure the image is in grayscale\n",
    "    if len(image.shape) != 2:\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Calculate gradients in the x and y directions\n",
    "    grad_x = cv2.Sobel(image, cv2.CV_64F, 1, 0, ksize=3)\n",
    "    grad_y = cv2.Sobel(image, cv2.CV_64F, 0, 1, ksize=3)\n",
    "    \n",
    "    # Calculate the magnitude of the gradients\n",
    "    grad_magnitude = np.sqrt(grad_x**2 + grad_y**2)\n",
    "    \n",
    "    # Calculate the average gradient\n",
    "    average_gradient = np.mean(grad_magnitude)\n",
    "    \n",
    "    return average_gradient\n",
    "\n",
    "# Load your fused image\n",
    "fused_img = cv2.imread('D:/SDP/Imfusion-main/Fused_Images/fused.png', cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "# Calculate the average gradient\n",
    "average_gradient_value = calculate_average_gradient(fused_img)\n",
    "print(f\"Average Gradient: {average_gradient_value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b546a9e-3cb9-4518-a8a2-0c8ae20b637b",
   "metadata": {},
   "source": [
    "Summary of Average Gradient:\n",
    "\n",
    "Value: An AG of 78.01 is generally considered high in the context of image processing, suggesting that the fused image retains a good amount of detail and texture from the original images.\n",
    "\n",
    "Interpretation: Higher AG values are indicative of sharp edges and clear textures, which are essential for detailed visualization in applications such as medical imaging. It suggests that the image fusion process has effectively combined the relevant features from both the CT and MRI scans into the fused image without losing significant detail.\n",
    "\n",
    "Conclusion: \n",
    "\n",
    "The fusion process appears to be successful in maintaining the structural integrity and detail from the source images. This is particularly important in medical imaging where the clarity of structural and textural details can be crucial for diagnosis and analysis.\n",
    "\n",
    "\n",
    "Overall Impression:\n",
    "\n",
    "Given the high Average Gradient value, the fusion technique used seems to effectively enhance or preserve the essential features from the original images, making the fused image potentially more useful for further analysis or clinical interpretation. This metric complements the entropy measure, providing a more comprehensive view of the fusion quality—where entropy assesses the information content, and average gradient evaluates the visual quality and sharpness.\n",
    "\n",
    "Combining these insights, the fused image not only contains a high level of information (as indicated by entropy) but also presents this information with clarity and detail (as indicated by the average gradient). This makes for a robust assessment of the fusion's effectiveness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f693576b-366f-4a00-a040-66b130341451",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edge-based Similarity Measurement (QAB/F): 642967.07013141\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def edge_similarity(image1, image2, fused_image):\n",
    "    \"\"\"Calculate the edge-based similarity measurement between source images and fused image.\"\"\"\n",
    "    # Compute gradients for source images and fused image\n",
    "    grad_x1, grad_y1 = cv2.Sobel(image1, cv2.CV_64F, 1, 0), cv2.Sobel(image1, cv2.CV_64F, 0, 1)\n",
    "    grad_x2, grad_y2 = cv2.Sobel(image2, cv2.CV_64F, 1, 0), cv2.Sobel(image2, cv2.CV_64F, 0, 1)\n",
    "    grad_xf, grad_yf = cv2.Sobel(fused_image, cv2.CV_64F, 1, 0), cv2.Sobel(fused_image, cv2.CV_64F, 0, 1)\n",
    "    \n",
    "    # Calculate gradient magnitudes and angles\n",
    "    magnitude1, angle1 = cv2.cartToPolar(grad_x1, grad_y1)\n",
    "    magnitude2, angle2 = cv2.cartToPolar(grad_x2, grad_y2)\n",
    "    magnitudef, anglef = cv2.cartToPolar(grad_xf, grad_yf)\n",
    "    \n",
    "    # Calculate edge information retention (intensity and orientation)\n",
    "    qa_f = magnitude1 * (1 - np.abs(np.sin(angle1 - anglef)))\n",
    "    qb_f = magnitude2 * (1 - np.abs(np.sin(angle2 - anglef)))\n",
    "    \n",
    "    # Define weights for each source image (example: equal weights)\n",
    "    wa = wb = 0.5  # Adjust these weights as per the importance of each source image\n",
    "    \n",
    "    # Calculate edge-based similarity measure\n",
    "    numerator = np.sum(qa_f * wa + qb_f * wb)\n",
    "    denominator = np.sum(wa + wb)\n",
    "    \n",
    "    qab_f = numerator / denominator if denominator != 0 else 0\n",
    "    \n",
    "    return qab_f\n",
    "    \n",
    "def resize_images(*images):\n",
    "    # Find the smallest dimensions in the set\n",
    "    min_height = min(image.shape[0] for image in images)\n",
    "    min_width = min(image.shape[1] for image in images)\n",
    "    resized_images = [cv2.resize(img, (min_width, min_height), interpolation=cv2.INTER_AREA) for img in images]\n",
    "    return resized_images\n",
    "\n",
    "# Load your images\n",
    "img1 = cv2.imread('D:/SDP/Imfusion-main/screenshots/medical1.png', cv2.IMREAD_GRAYSCALE)\n",
    "img2 = cv2.imread('D:/SDP/Imfusion-main/screenshots/medical2.png', cv2.IMREAD_GRAYSCALE)\n",
    "fused_img = cv2.imread('D:/SDP/Imfusion-main/Fused_Images/fused.png', cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "\n",
    "img1, img2, fused_img = resize_images(img1, img2, fused_img)\n",
    "\n",
    "# Calculate the edge-based similarity measurement\n",
    "qab_f_value = edge_similarity(img1, img2, fused_img)\n",
    "print(f\"Edge-based Similarity Measurement (QAB/F): {qab_f_value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21915298-fdf3-473d-8692-2a5712d1348f",
   "metadata": {},
   "source": [
    "The Edge-based Similarity Measurement (QAB/F) for your fused image is 642967.07, which is a notably high value. This indicates a strong retention of edge information from the original CT and MRI scans in the fused image. Here's a detailed interpretation and summary of this result:\n",
    "\n",
    "Summary of Edge-based Similarity Measurement:\n",
    "\n",
    "Value: A QAB/F value of approximately 642967 suggests that the edges from the source images are very well preserved in the fused image.\n",
    "\n",
    "Interpretation: Edge information is critical in medical imaging as it helps in delineating structures and identifying features essential for accurate diagnosis. High edge retention indicates that the fusion process has successfully integrated the important features from both source images without significant degradation or loss of detail.\n",
    "\n",
    "Conclusion: The fusion technique used is highly effective in terms of preserving structural integrity through edges, which is crucial for the visual clarity and utility of the image in clinical settings. This suggests that the fused image maintains a high level of detail, making it valuable for further analysis or interpretation in medical applications.\n",
    "\n",
    "\n",
    "Overall Impression:\n",
    "\n",
    "Given the high QAB/F value, the fusion process appears to be exceptionally successful in combining the CT and MRI images in a way that maintains their most critical visual information—edges. This metric, alongside the previously calculated entropy and average gradient, offers a comprehensive view of the fusion quality. High entropy and average gradient, coupled with excellent edge retention, denote a fusion outcome that is likely to be of high quality and utility.\n",
    "\n",
    "These metrics collectively suggest that the fused image not only contains a significant amount of information but also presents it with clarity and precision, making it a potentially effective tool in medical imaging scenarios where detailed and accurate visual representations are necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1584391-8961-497b-8d56-cd21945b55a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard Deviation: 46.81556904792552\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def calculate_standard_deviation(image):\n",
    "    \"\"\"Calculate the standard deviation of an image.\"\"\"\n",
    "    # Ensure the image is in grayscale\n",
    "    if len(image.shape) != 2:\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Calculate mean\n",
    "    mean_value = np.mean(image)\n",
    "    # Calculate standard deviation\n",
    "    standard_deviation = np.sqrt(np.mean((image - mean_value) ** 2))\n",
    "    \n",
    "    return standard_deviation\n",
    "\n",
    "# Load your fused image\n",
    "fused_img = cv2.imread('D:/SDP/Imfusion-main/Fused_Images/fused.png', cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "# Calculate the standard deviation\n",
    "standard_deviation_value = calculate_standard_deviation(fused_img)\n",
    "print(f\"Standard Deviation: {standard_deviation_value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d18f5b9-b6b0-4f4b-8612-37a2ded00c3d",
   "metadata": {},
   "source": [
    "Summary of Standard Deviation:\n",
    "\n",
    "Value: An SD of 46.82 is relatively high, indicating a significant level of contrast within the fused image.\n",
    "\n",
    "Interpretation: High standard deviation in an image implies that there is a wide range of pixel intensities. This diversity in pixel values generally enhances the image's contrast, making the details within the image more distinguishable and the overall image more dynamic and visually informative.\n",
    "\n",
    "Conclusion: \n",
    "\n",
    "The higher contrast denoted by the substantial SD value suggests that the fusion process effectively enhances or maintains the dynamic range of the original images. This is desirable in many applications, particularly in medical imaging, where contrast plays a crucial role in the visibility of various anatomical structures and abnormalities.\n",
    "\n",
    "\n",
    "Overall Impression:\n",
    "\n",
    "The higher Standard Deviation value indicates that the fusion process used is successful in retaining or even enhancing the contrast and dynamic range from the original CT and MRI scans. This contrast is crucial for ensuring that important features within the images are pronounced and easily discernible, which can be vital for subsequent analysis, diagnosis, or any automated processing tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c07e793e-b128-4ee4-886a-d291979280e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spatial Frequency: 28.85894511880092\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def calculate_spatial_frequency(image):\n",
    "    \"\"\"Calculate the spatial frequency of an image.\"\"\"\n",
    "    # Ensure the image is in grayscale\n",
    "    if len(image.shape) != 2:\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Calculate differences between adjacent pixels along rows and columns\n",
    "    row_diff = np.diff(image.astype('float'), axis=0)\n",
    "    col_diff = np.diff(image.astype('float'), axis=1)\n",
    "    \n",
    "    # Calculate the squares of differences\n",
    "    RF_squared = np.sum(row_diff ** 2)\n",
    "    CF_squared = np.sum(col_diff ** 2)\n",
    "    \n",
    "    # Calculate row frequency (RF) and column frequency (CF)\n",
    "    RF = np.sqrt(RF_squared / (image.shape[0] * image.shape[1]))\n",
    "    CF = np.sqrt(CF_squared / (image.shape[0] * image.shape[1]))\n",
    "    \n",
    "    # Calculate spatial frequency (SF)\n",
    "    SF = np.sqrt(RF ** 2 + CF ** 2)\n",
    "    \n",
    "    return SF\n",
    "\n",
    "# Load your fused image\n",
    "fused_img = cv2.imread('D:/SDP/Imfusion-main/Fused_Images/fused.png', cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "# Calculate the spatial frequency\n",
    "spatial_frequency_value = calculate_spatial_frequency(fused_img)\n",
    "print(f\"Spatial Frequency: {spatial_frequency_value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9786be8-b4c0-4e79-b0de-2a2e487e823c",
   "metadata": {},
   "source": [
    "Summary of Spatial Frequency:\n",
    "\n",
    "Value: An SF of 28.86 suggests that there is a significant amount of texture and edge detail in the fused image. This is a relatively high value that points towards an active variation in pixel intensity across the image.\n",
    "\n",
    "Interpretation: Spatial Frequency measures the variations in pixel intensities which correspond to edges and textural details. A higher SF value generally indicates that these details are prevalent and well-preserved or even enhanced in the fused image. This is beneficial for visual clarity and the usefulness of the image in applications where detail perception is crucial.\n",
    "\n",
    "Conclusion: \n",
    "\n",
    "The fusion process appears to be effective in maintaining or enhancing the textural and edge details from the original CT and MRI scans. This suggests that the fused image not only retains important information from both source images but also presents this information in a manner that is visually rich and detailed.\n",
    "\n",
    "\n",
    "Overall Impression:\n",
    "\n",
    "\n",
    "Given the calculated metrics of entropy, average gradient, edge-based similarity, standard deviation, and now spatial frequency, your fused image maintains a high level of information content, edge detail, and textural complexity. These characteristics are indicative of a high-quality image fusion process that enhances the utility of the resulting image for detailed analysis or diagnostic purposes. The higher spatial frequency specifically confirms that the image is likely to be more effective in applications that rely on detailed visual inspections, such as medical imaging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c854365d-b82f-41e9-81e1-53db9c8c6ad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edge Intensity: 14363.941381111244\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def calculate_edge_intensity(image):\n",
    "    \"\"\"Calculate the edge intensity of an image using Sobel operators.\"\"\"\n",
    "    # Ensure the image is in grayscale\n",
    "    if len(image.shape) != 2:\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Sobel operators to find horizontal and vertical gradients\n",
    "    sobel_x = cv2.Sobel(image, cv2.CV_64F, 1, 0, ksize=3)\n",
    "    sobel_y = cv2.Sobel(image, cv2.CV_64F, 0, 1, ksize=3)\n",
    "    \n",
    "    # Calculate the squares of gradients\n",
    "    S2x = np.sum(sobel_x**2)\n",
    "    S2y = np.sum(sobel_y**2)\n",
    "    \n",
    "    # Calculate Edge Intensity\n",
    "    EI = np.sqrt(S2x + S2y)\n",
    "    \n",
    "    return EI\n",
    "\n",
    "# Load your fused image\n",
    "fused_img = cv2.imread('D:/SDP/Imfusion-main/Fused_Images/fused.png', cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "# Calculate the edge intensity\n",
    "edge_intensity_value = calculate_edge_intensity(fused_img)\n",
    "print(f\"Edge Intensity: {edge_intensity_value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f734ca25-6072-458d-a7cd-c76c0f97ec36",
   "metadata": {},
   "source": [
    "Summary of Edge Intensity:\n",
    "\n",
    "Value: An EI of approximately 14363.94 is significantly high, reflecting a strong presence of edge details in the fused image.\n",
    "\n",
    "Interpretation: Edge intensity is a crucial metric in evaluating the quality of image fusion, especially for its ability to highlight how well the fusion process preserves or enhances the edges from the source images. Edges are essential for delineating boundaries and features within the image, and a higher edge intensity indicates that these features are both clear and prominent.\n",
    "\n",
    "Conclusion: \n",
    "\n",
    "The high edge intensity value suggests that the fusion technique effectively consolidates and even amplifies edge information from the CT and MRI scans. This ensures that the fused image maintains high clarity, making it more useful for applications that rely heavily on edge detail for analysis, such as medical imaging diagnostics or feature-based geospatial analysis.\n",
    "\n",
    "\n",
    "Overall Impression:\n",
    "\n",
    "Given the calculated edge intensity, along with the previous metrics like entropy, average gradient, edge-based similarity measurement, standard deviation, and spatial frequency, it is evident that the fused image is of high quality. The consistency in high metric values across different measures—reflecting detail retention, information content, and visual clarity—indicates a successful image fusion process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc54490-4ef4-49f7-90c1-8d85a5bc9da1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
